{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12558099,"sourceType":"datasetVersion","datasetId":7929695},{"sourceId":12558172,"sourceType":"datasetVersion","datasetId":7929746},{"sourceId":12570718,"sourceType":"datasetVersion","datasetId":7938501}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport csv\nimport glob\nimport torch\nimport multiprocessing\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.transforms as transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.694802Z","iopub.execute_input":"2025-07-25T05:08:40.695064Z","iopub.status.idle":"2025-07-25T05:08:40.707907Z","shell.execute_reply.started":"2025-07-25T05:08:40.695042Z","shell.execute_reply":"2025-07-25T05:08:40.707228Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.708974Z","iopub.execute_input":"2025-07-25T05:08:40.709171Z","iopub.status.idle":"2025-07-25T05:08:40.721723Z","shell.execute_reply.started":"2025-07-25T05:08:40.709156Z","shell.execute_reply":"2025-07-25T05:08:40.721238Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# @title Set device (GPU or CPU)\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.722408Z","iopub.execute_input":"2025-07-25T05:08:40.722657Z","iopub.status.idle":"2025-07-25T05:08:40.733470Z","shell.execute_reply.started":"2025-07-25T05:08:40.722637Z","shell.execute_reply":"2025-07-25T05:08:40.732863Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"set_seed(seed=2021)\ndevice = set_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.734963Z","iopub.execute_input":"2025-07-25T05:08:40.735172Z","iopub.status.idle":"2025-07-25T05:08:40.833648Z","shell.execute_reply.started":"2025-07-25T05:08:40.735159Z","shell.execute_reply":"2025-07-25T05:08:40.832969Z"}},"outputs":[{"name":"stdout","text":"Random seed 2021 has been set.\nGPU is enabled in this notebook.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nuse_cuda = torch.cuda.is_available()\nstart_epoch = 0\nbest_acc = 0  # best test accuracy\nbatch_size = 128\nmax_epochs_target = 40\nbase_learning_rate = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.834392Z","iopub.execute_input":"2025-07-25T05:08:40.834641Z","iopub.status.idle":"2025-07-25T05:08:40.838498Z","shell.execute_reply.started":"2025-07-25T05:08:40.834620Z","shell.execute_reply":"2025-07-25T05:08:40.837819Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# @markdown Download and prepare Source Data\n##print('==> Preparing data..')\ndef percentageSplit(full_dataset, percent = 0.0):\n  set1_size = int(percent * len(full_dataset))\n  set2_size = len(full_dataset) - set1_size\n  final_dataset, _ = torch.utils.data.random_split(full_dataset, [set1_size, set2_size])\n  return final_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.839115Z","iopub.execute_input":"2025-07-25T05:08:40.839332Z","iopub.status.idle":"2025-07-25T05:08:40.850352Z","shell.execute_reply.started":"2025-07-25T05:08:40.839317Z","shell.execute_reply":"2025-07-25T05:08:40.849773Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# ResNet","metadata":{}},{"cell_type":"code","source":"# @title ResNet model in PyTorch\n\nclass BasicBlock(nn.Module):\n  \"\"\"ResNet in PyTorch.\n      Reference:\n      [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n        Deep Residual Learning for Image Recognition. arXiv:1512.03385\n  \"\"\"\n\n  expansion = 1\n\n  def __init__(self, in_planes, planes, stride=1):\n    super(BasicBlock, self).__init__()\n    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(planes)\n\n    self.shortcut = nn.Sequential()\n    if stride != 1 or in_planes != self.expansion*planes:\n        self.shortcut = nn.Sequential(\n            nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(self.expansion*planes)\n        )\n\n  def forward(self, x):\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = self.bn2(self.conv2(out))\n    out += self.shortcut(x)\n    out = F.relu(out)\n    return out\n\n\nclass Bottleneck(nn.Module):\n  expansion = 4\n\n  def __init__(self, in_planes, planes, stride=1):\n    super(Bottleneck, self).__init__()\n    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(planes)\n    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n    self.shortcut = nn.Sequential()\n    if stride != 1 or in_planes != self.expansion*planes:\n        self.shortcut = nn.Sequential(\n            nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(self.expansion*planes)\n        )\n\n  def forward(self, x):\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = F.relu(self.bn2(self.conv2(out)))\n    out = self.bn3(self.conv3(out))\n    out += self.shortcut(x)\n    out = F.relu(out)\n    return out\n\n\nclass ResNet(nn.Module):\n  def __init__(self, block, num_blocks, num_classes=100):\n    super(ResNet, self).__init__()\n    self.in_planes = 64\n\n    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn1 = nn.BatchNorm2d(64)\n    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n    self.linear = nn.Linear(512*block.expansion, num_classes)\n\n  def _make_layer(self, block, planes, num_blocks, stride):\n    strides = [stride] + [1]*(num_blocks-1)\n    layers = []\n    for stride in strides:\n      layers.append(block(self.in_planes, planes, stride))\n      self.in_planes = planes * block.expansion\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = self.layer1(out)\n    out = self.layer2(out)\n    out = self.layer3(out)\n    out = self.layer4(out)\n    out = F.avg_pool2d(out, 4)\n    out = out.view(out.size(0), -1)\n    out = self.linear(out)\n    return out\n\n\ndef ResNet18():\n  return ResNet(BasicBlock, [2, 2, 2, 2])\n\n\ndef ResNet34():\n  return ResNet(BasicBlock, [3, 4, 6, 3])\n\n\ndef ResNet50(num_classes=100):\n  return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.865975Z","iopub.execute_input":"2025-07-25T05:08:40.866187Z","iopub.status.idle":"2025-07-25T05:08:40.882001Z","shell.execute_reply.started":"2025-07-25T05:08:40.866159Z","shell.execute_reply":"2025-07-25T05:08:40.881392Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef checkpoint(model, acc, epoch, outModelName):\n  # Save checkpoint.\n  print('Saving..')\n  state = {\n      'state_dict': model.state_dict(),\n      'acc': acc,\n      'epoch': epoch,\n      'rng_state': torch.get_rng_state()\n  }\n  if not os.path.isdir('checkpoint'):\n      os.mkdir('checkpoint')\n  torch.save(state, f'./checkpoint/{outModelName}.t7')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.906378Z","iopub.execute_input":"2025-07-25T05:08:40.906633Z","iopub.status.idle":"2025-07-25T05:08:40.920243Z","shell.execute_reply.started":"2025-07-25T05:08:40.906603Z","shell.execute_reply":"2025-07-25T05:08:40.919571Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Train and test functions","metadata":{}},{"cell_type":"code","source":"def train(model, loader, epoch, optimizer, criterion, use_cuda=True):\n  print('\\nEpoch: %d' % epoch)\n  print(f\"LR head / body : {[g['lr'] for g in optimizer.param_groups]} ===\")\n  print(f'Using {len(loader.dataset)} data during training')\n  model.train()\n  train_loss = 0\n  correct = 0\n  total = 0\n  for batch_idx, (inputs, targets) in enumerate(loader):\n    if use_cuda:\n      inputs, targets = inputs.cuda(), targets.cuda()\n\n    optimizer.zero_grad()\n    inputs, targets = Variable(inputs), Variable(targets)\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    optimizer.step()\n\n    train_loss += loss.item()\n    _, predicted = torch.max(outputs.data, 1)\n    total += targets.size(0)\n    correct += predicted.eq(targets.data).cpu().sum()\n\n    if batch_idx % 500 == 0:\n      print(batch_idx, len(loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n          % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n  return (train_loss/batch_idx, 100.*correct/total)\n\n\ndef test(model, loader, epoch, outModelName, criterion, use_cuda=True):\n  global best_acc\n  model.eval()\n  test_loss, correct, total = 0, 0, 0\n  with torch.no_grad():\n    for batch_idx, (inputs, targets) in enumerate(loader):\n      if use_cuda:\n        inputs, targets = inputs.cuda(), targets.cuda()\n\n      outputs = model(inputs)\n      loss = criterion(outputs, targets)\n\n      test_loss += loss.item()\n      _, predicted = torch.max(outputs.data, 1)\n      total += targets.size(0)\n      correct += predicted.eq(targets.data).cpu().sum()\n\n      if batch_idx % 200 == 0:\n        print(batch_idx, len(loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n  # Save checkpoint.\n  acc = 100.*correct/total\n  if acc > best_acc:\n    best_acc = acc\n    checkpoint(model, acc, epoch, outModelName)\n  return (test_loss/batch_idx, 100.*correct/total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.933408Z","iopub.execute_input":"2025-07-25T05:08:40.933635Z","iopub.status.idle":"2025-07-25T05:08:40.943541Z","shell.execute_reply.started":"2025-07-25T05:08:40.933616Z","shell.execute_reply":"2025-07-25T05:08:40.942932Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Check number of nodes in last layer\n# This will keep updating until it finds the last Linear layer in the hierarchy\n\ndef check_num_features(model):\n\n  # Iterating through named modules to find the last Linear layer\n  output_nodes = None\n  for name, module in model.named_modules():\n      if isinstance(module, nn.Linear):\n          output_nodes = module.out_features\n\n  if output_nodes is not None:\n      print(f\"Number of nodes in the output layer (found by iteration): {output_nodes}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.969660Z","iopub.execute_input":"2025-07-25T05:08:40.969864Z","iopub.status.idle":"2025-07-25T05:08:40.979588Z","shell.execute_reply.started":"2025-07-25T05:08:40.969844Z","shell.execute_reply":"2025-07-25T05:08:40.978865Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"'''Freezing / unfreezing functions '''\ndef freeze_all(model):\n    print('freezing all')\n    for p in model.parameters():\n        p.requires_grad = False\n\ndef unfreeze_head(model):\n    for p in model.linear.parameters():\n        p.requires_grad = True\n\ndef set_bn_eval(model):\n    \"\"\"Set BatchNorm to eval to prevent stats updates when backbone frozen.\"\"\"\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.eval()\n\ndef unfreeze_all(model):\n    for p in model.parameters():\n        p.requires_grad = True\n\ndef unfreeze_layer4(model):\n    for p in model.layer4.parameters():\n        p.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:40.991811Z","iopub.execute_input":"2025-07-25T05:08:40.992043Z","iopub.status.idle":"2025-07-25T05:08:41.004847Z","shell.execute_reply.started":"2025-07-25T05:08:40.992023Z","shell.execute_reply":"2025-07-25T05:08:41.004315Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def set_optimizer(model, head_lr, body_lr, wd=5e-4):\n    head, body = [], []\n    for n, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        if n.startswith(\"linear\") or \"linear\" in n:  # suffit pour ta classe\n            head.append(p)\n        else:\n            body.append(p)\n    param_groups = []\n    if head:\n        param_groups.append({\"params\": head, \"lr\": head_lr})\n    if body_lr and body:\n        param_groups.append({\"params\": body, \"lr\": body_lr})\n    return optim.SGD(param_groups, momentum=0.9, weight_decay=wd, nesterov=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:41.005495Z","iopub.execute_input":"2025-07-25T05:08:41.005669Z","iopub.status.idle":"2025-07-25T05:08:41.015781Z","shell.execute_reply.started":"2025-07-25T05:08:41.005657Z","shell.execute_reply":"2025-07-25T05:08:41.015253Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#Three-phase finetuning of model on target data \ndef setup_phase_head(model, lr=1e-2):\n    \"\"\"Phase 1 : train only head(linear).\"\"\"\n    freeze_all(model)\n    unfreeze_head(model)\n    set_bn_eval(model)\n    return set_optimizer(model, head_lr=lr, body_lr=0)\n\n\ndef setup_phase_layer4(model):\n    \"\"\"Phase 2 : train head + layer4 (before last layer).\"\"\"\n    freeze_all(model)\n    unfreeze_head(model)\n    unfreeze_layer4(model)\n    set_bn_eval(model)\n    return set_optimizer(model, head_lr=5e-3, body_lr=5e-4)\n\n\ndef setup_phase_full(model, update_bn=True):\n    \"\"\"Phase 3 : full fine-tuning .\n        \"\"\"\n    unfreeze_all(model)\n    if not update_bn:\n        set_bn_eval(model)   # keep stats BN from pretrain\n    return set_optimizer(model, head_lr=1e-4, body_lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:41.016391Z","iopub.execute_input":"2025-07-25T05:08:41.016599Z","iopub.status.idle":"2025-07-25T05:08:41.026400Z","shell.execute_reply.started":"2025-07-25T05:08:41.016581Z","shell.execute_reply":"2025-07-25T05:08:41.025706Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"PHASES = [\n    {\"name\": \"head\",   \"epochs\": 10,  \"setup_fn\": setup_phase_head},\n    {\"name\": \"layer4\", \"epochs\": 10, \"setup_fn\": setup_phase_layer4},\n    {\"name\": \"full\",   \"epochs\": 20, \"setup_fn\": lambda m: setup_phase_full(m, update_bn=True)},\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:41.027007Z","iopub.execute_input":"2025-07-25T05:08:41.027200Z","iopub.status.idle":"2025-07-25T05:08:41.039482Z","shell.execute_reply.started":"2025-07-25T05:08:41.027187Z","shell.execute_reply":"2025-07-25T05:08:41.038882Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#refresh last layer of network for new task\ndef replace_classification_layer(model,num_classes):\n  #Replace classifier head for new task\n  num_ftrs = model.linear.in_features\n  model.linear = nn.Linear(num_ftrs, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:41.040110Z","iopub.execute_input":"2025-07-25T05:08:41.040289Z","iopub.status.idle":"2025-07-25T05:08:41.050653Z","shell.execute_reply.started":"2025-07-25T05:08:41.040276Z","shell.execute_reply":"2025-07-25T05:08:41.050107Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"#get balanced samples when taking a percentage of training set\nfrom collections import defaultdict,Counter\n\ndef get_stratified_loader(dataset, fraction, batch_size=128):\n    num_samples = int(len(dataset) * fraction)\n    targets = [dataset[i][1] for i in range(len(dataset))]\n\n    # Group indices by class\n    class_to_indices = defaultdict(list)\n    for idx, label in enumerate(targets):\n        class_to_indices[label].append(idx)\n\n    selected_indices = []\n\n    for cls, idxs in class_to_indices.items():\n        n = num_samples // num_classes  # or n = num_samples // num_classes\n        selected = np.random.choice(idxs, n, replace=False)\n        selected_indices.extend(selected)\n\n    np.random.shuffle(selected_indices)\n    subset = torch.utils.data.Subset(dataset, selected_indices)\n    return torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:41.051546Z","iopub.execute_input":"2025-07-25T05:08:41.051713Z","iopub.status.idle":"2025-07-25T05:08:41.062844Z","shell.execute_reply.started":"2025-07-25T05:08:41.051694Z","shell.execute_reply":"2025-07-25T05:08:41.062094Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Target domain Data\nprint('==> Preparing target domain data..')\n\n# CIFAR10 normalizing\nmean = (0.4914, 0.4822, 0.4465)\nstd = (0.2023, 0.1994, 0.2010)\nnum_classes = 10\nlr = 0.0001\n\n# torchvision transforms\ntransform_train = transforms.Compose([])\nif torchvision_transforms:\n    transform_train.transforms.append(transforms.RandomCrop(32, padding=4))\n    transform_train.transforms.append(transforms.RandomHorizontalFlip())\n\ntransform_train.transforms.append(transforms.ToTensor())\ntransform_train.transforms.append(transforms.Normalize(mean, std))\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./CIFAR10', train=True, download=True, transform=transform_train)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./CIFAR10', train=False, download=True, transform=transform_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:41.063533Z","iopub.execute_input":"2025-07-25T05:08:41.063717Z","iopub.status.idle":"2025-07-25T05:08:44.191718Z","shell.execute_reply.started":"2025-07-25T05:08:41.063697Z","shell.execute_reply":"2025-07-25T05:08:44.191149Z"}},"outputs":[{"name":"stdout","text":"==> Preparing target domain data..\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"#train with x% of training data\n\ndef frac_data_train(name, pretrained_path, fraction, trainloader, testloader):\n    ''' name: file name to save output\n        pretrained_path: path of pretrained model\n        fraction: fraction of data to train with\n        trainloader: dataloader for train data\n        testloader: dataloader for test data\n    '''\n    result_folder = './results/'\n    if not os.path.exists(result_folder):\n        os.makedirs(result_folder)\n\n    criterion = nn.CrossEntropyLoss()\n    model = ResNet50()\n    # load pretrained model\n    if os.path.isfile(pretrained_path):\n        state_dict = torch.load(pretrained_path)\n        best_acc = state_dict['acc']\n        print('Best Accuracy:', best_acc)\n        if \"state_dict\" in state_dict:\n            state_dict = state_dict[\"state_dict\"]\n        # remove prefix \"module.\"\n        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n        for k, v in model.state_dict().items():\n            if k not in list(state_dict):\n                print('key \"{}\" could not be found in provided state dict'.format(k))\n            elif state_dict[k].shape != v.shape:\n                print('key \"{}\" is of different shape in model and provided state dict'.format(k))\n                state_dict[k] = v\n        msg = model.load_state_dict(state_dict, strict=False)\n        print(\"Load pretrained model with msg: {}\".format(msg))\n    else:\n        raise Exception('No pretrained weights found')\n\n    # outModelName = 'finetuned_'+str(fraction)\n    outModelName = name\n    logname = result_folder + model.__class__.__name__ + f'_{outModelName}.csv'\n    replace_classification_layer(model, 10)\n\n    model.to(device)\n\n    if not os.path.exists(logname):\n        with open(logname, 'w') as logfile:\n            logwriter = csv.writer(logfile, delimiter=',')\n            logwriter.writerow(['epoch', 'train loss', 'train acc', 'val loss', 'val acc'])\n\n    print(len(trainloader.dataset))\n\n    epoch_counter = 0\n\n    for phase_id, phase in enumerate(PHASES, 1):\n        optimizer = phase[\"setup_fn\"](model)\n        print(f\"\\n=== Phase {phase_id} | LR head / body : {[g['lr'] for g in optimizer.param_groups]} ===\")\n        for epoch in range(phase[\"epochs\"]):\n            train_loss, train_acc = train(model, trainloader, epoch_counter, optimizer, criterion, use_cuda=use_cuda)\n            # creates a checkpoint\n            val_loss, val_acc = test(model, testloader, epoch_counter, outModelName, criterion, use_cuda=use_cuda)\n            with open(logname, 'a') as logfile:\n                logwriter = csv.writer(logfile, delimiter=',')\n                logwriter.writerow([epoch_counter, train_loss, train_acc.item(), val_loss, val_acc.item()])\n            print(f'Epoch: {epoch_counter} | train acc: {train_acc} | val acc: {val_acc}')\n\n            epoch_counter += 1\n    del model, optimizer, trainloader, testloader\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:44.192426Z","iopub.execute_input":"2025-07-25T05:08:44.192624Z","iopub.status.idle":"2025-07-25T05:08:44.202829Z","shell.execute_reply.started":"2025-07-25T05:08:44.192610Z","shell.execute_reply":"2025-07-25T05:08:44.202244Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"batch_size = 128\nfractions = [0.01,0.2,0.4,0.6,1.0]\n\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=batch_size, shuffle=False, num_workers=2)\nfor fraction in fractions:\n    trainloader = get_stratified_loader(trainset, fraction)\n    print(len(trainloader.dataset))\n    all_labels = []\n    for _, labels in trainloader:\n        all_labels.extend(labels.tolist())\n    \n    class_counts = Counter(all_labels)\n    total = sum(class_counts.values())\n    \n    for cls, count in sorted(class_counts.items()):\n        print(f\"Class {cls}: {count} samples ({100 * count / total:.2f}%)\")\n    frac_data_train('finetuned_'+str(fraction),'/kaggle/input/pretrained-model/pretrain.t7',fraction, trainloader,testloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:08:44.203489Z","iopub.execute_input":"2025-07-25T05:08:44.203674Z","iopub.status.idle":"2025-07-25T05:16:02.439666Z","shell.execute_reply.started":"2025-07-25T05:08:44.203660Z","shell.execute_reply":"2025-07-25T05:16:02.438941Z"}},"outputs":[{"name":"stdout","text":"50000 500\n500\nClass 0: 50 samples (10.00%)\nClass 1: 50 samples (10.00%)\nClass 2: 50 samples (10.00%)\nClass 3: 50 samples (10.00%)\nClass 4: 50 samples (10.00%)\nClass 5: 50 samples (10.00%)\nClass 6: 50 samples (10.00%)\nClass 7: 50 samples (10.00%)\nClass 8: 50 samples (10.00%)\nClass 9: 50 samples (10.00%)\nBest Accuracy: tensor(74.4000)\nLoad pretrained model with msg: <All keys matched successfully>\n500\nfreezing all\n\n=== Phase 1 | LR head / body : [0.01] ===\n\nEpoch: 0\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 2.353 | Acc: 14.062% (18/128)\n0 79 Loss: 1.827 | Acc: 27.344% (35/128)\nSaving..\nEpoch: 0 | train acc: 16.0 | val acc: 29.059999465942383\n\nEpoch: 1\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 1.806 | Acc: 33.594% (43/128)\n0 79 Loss: 1.376 | Acc: 59.375% (76/128)\nSaving..\nEpoch: 1 | train acc: 49.0 | val acc: 55.939998626708984\n\nEpoch: 2\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 1.210 | Acc: 67.969% (87/128)\n0 79 Loss: 1.027 | Acc: 63.281% (81/128)\nSaving..\nEpoch: 2 | train acc: 71.5999984741211 | val acc: 62.970001220703125\n\nEpoch: 3\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.814 | Acc: 78.125% (100/128)\n0 79 Loss: 0.925 | Acc: 68.750% (88/128)\nSaving..\nEpoch: 3 | train acc: 74.0 | val acc: 65.2699966430664\n\nEpoch: 4\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.636 | Acc: 82.031% (105/128)\n0 79 Loss: 0.895 | Acc: 68.750% (88/128)\nSaving..\nEpoch: 4 | train acc: 77.80000305175781 | val acc: 65.45999908447266\n\nEpoch: 5\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.680 | Acc: 75.781% (97/128)\n0 79 Loss: 0.845 | Acc: 72.656% (93/128)\nSaving..\nEpoch: 5 | train acc: 80.80000305175781 | val acc: 66.45999908447266\n\nEpoch: 6\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.577 | Acc: 82.812% (106/128)\n0 79 Loss: 0.808 | Acc: 67.969% (87/128)\nSaving..\nEpoch: 6 | train acc: 83.80000305175781 | val acc: 67.12000274658203\n\nEpoch: 7\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.468 | Acc: 84.375% (108/128)\n0 79 Loss: 0.844 | Acc: 66.406% (85/128)\nEpoch: 7 | train acc: 83.80000305175781 | val acc: 66.9000015258789\n\nEpoch: 8\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.506 | Acc: 89.062% (114/128)\n0 79 Loss: 0.810 | Acc: 70.312% (90/128)\nEpoch: 8 | train acc: 87.19999694824219 | val acc: 66.62999725341797\n\nEpoch: 9\nLR head / body : [0.01] ===\nUsing 500 data during training\n0 4 Loss: 0.333 | Acc: 91.406% (117/128)\n0 79 Loss: 0.900 | Acc: 65.625% (84/128)\nEpoch: 9 | train acc: 87.4000015258789 | val acc: 66.22000122070312\nfreezing all\n\n=== Phase 2 | LR head / body : [0.005, 0.0005] ===\n\nEpoch: 10\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.422 | Acc: 89.844% (115/128)\n0 79 Loss: 0.788 | Acc: 69.531% (89/128)\nEpoch: 10 | train acc: 88.80000305175781 | val acc: 66.55000305175781\n\nEpoch: 11\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.366 | Acc: 90.625% (116/128)\n0 79 Loss: 0.806 | Acc: 69.531% (89/128)\nEpoch: 11 | train acc: 90.19999694824219 | val acc: 66.80999755859375\n\nEpoch: 12\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.366 | Acc: 94.531% (121/128)\n0 79 Loss: 0.811 | Acc: 69.531% (89/128)\nEpoch: 12 | train acc: 91.19999694824219 | val acc: 66.91000366210938\n\nEpoch: 13\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.325 | Acc: 94.531% (121/128)\n0 79 Loss: 0.802 | Acc: 71.094% (91/128)\nEpoch: 13 | train acc: 92.4000015258789 | val acc: 66.8499984741211\n\nEpoch: 14\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.258 | Acc: 96.094% (123/128)\n0 79 Loss: 0.796 | Acc: 70.312% (90/128)\nEpoch: 14 | train acc: 93.4000015258789 | val acc: 66.69000244140625\n\nEpoch: 15\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.326 | Acc: 92.969% (119/128)\n0 79 Loss: 0.805 | Acc: 71.094% (91/128)\nEpoch: 15 | train acc: 94.19999694824219 | val acc: 66.73999786376953\n\nEpoch: 16\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.255 | Acc: 96.875% (124/128)\n0 79 Loss: 0.804 | Acc: 70.312% (90/128)\nEpoch: 16 | train acc: 94.19999694824219 | val acc: 66.93000030517578\n\nEpoch: 17\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.257 | Acc: 96.094% (123/128)\n0 79 Loss: 0.800 | Acc: 72.656% (93/128)\nEpoch: 17 | train acc: 95.0 | val acc: 66.86000061035156\n\nEpoch: 18\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.262 | Acc: 96.875% (124/128)\n0 79 Loss: 0.801 | Acc: 71.094% (91/128)\nEpoch: 18 | train acc: 97.0 | val acc: 66.55999755859375\n\nEpoch: 19\nLR head / body : [0.005, 0.0005] ===\nUsing 500 data during training\n0 4 Loss: 0.229 | Acc: 96.875% (124/128)\n0 79 Loss: 0.819 | Acc: 70.312% (90/128)\nEpoch: 19 | train acc: 95.80000305175781 | val acc: 66.9000015258789\n\n=== Phase 3 | LR head / body : [0.0001, 0.0001] ===\n\nEpoch: 20\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.243 | Acc: 98.438% (126/128)\n0 79 Loss: 0.819 | Acc: 70.312% (90/128)\nEpoch: 20 | train acc: 98.0 | val acc: 66.91000366210938\n\nEpoch: 21\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.227 | Acc: 97.656% (125/128)\n0 79 Loss: 0.816 | Acc: 70.312% (90/128)\nEpoch: 21 | train acc: 97.4000015258789 | val acc: 66.87999725341797\n\nEpoch: 22\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.231 | Acc: 96.875% (124/128)\n0 79 Loss: 0.814 | Acc: 70.312% (90/128)\nEpoch: 22 | train acc: 97.5999984741211 | val acc: 66.86000061035156\n\nEpoch: 23\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.236 | Acc: 97.656% (125/128)\n0 79 Loss: 0.813 | Acc: 70.312% (90/128)\nEpoch: 23 | train acc: 98.0 | val acc: 66.88999938964844\n\nEpoch: 24\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.213 | Acc: 98.438% (126/128)\n0 79 Loss: 0.810 | Acc: 70.312% (90/128)\nEpoch: 24 | train acc: 98.19999694824219 | val acc: 66.95999908447266\n\nEpoch: 25\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.197 | Acc: 99.219% (127/128)\n0 79 Loss: 0.810 | Acc: 70.312% (90/128)\nEpoch: 25 | train acc: 98.19999694824219 | val acc: 66.97000122070312\n\nEpoch: 26\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.262 | Acc: 96.875% (124/128)\n0 79 Loss: 0.806 | Acc: 71.875% (92/128)\nEpoch: 26 | train acc: 97.4000015258789 | val acc: 66.9800033569336\n\nEpoch: 27\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.211 | Acc: 98.438% (126/128)\n0 79 Loss: 0.804 | Acc: 71.875% (92/128)\nEpoch: 27 | train acc: 99.0 | val acc: 66.98999786376953\n\nEpoch: 28\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.246 | Acc: 97.656% (125/128)\n0 79 Loss: 0.803 | Acc: 71.875% (92/128)\nEpoch: 28 | train acc: 98.19999694824219 | val acc: 67.01000213623047\n\nEpoch: 29\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.243 | Acc: 96.875% (124/128)\n0 79 Loss: 0.803 | Acc: 71.875% (92/128)\nEpoch: 29 | train acc: 97.80000305175781 | val acc: 66.9800033569336\n\nEpoch: 30\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.195 | Acc: 99.219% (127/128)\n0 79 Loss: 0.801 | Acc: 71.875% (92/128)\nEpoch: 30 | train acc: 98.19999694824219 | val acc: 66.93000030517578\n\nEpoch: 31\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.200 | Acc: 99.219% (127/128)\n0 79 Loss: 0.801 | Acc: 71.875% (92/128)\nEpoch: 31 | train acc: 99.0 | val acc: 66.9800033569336\n\nEpoch: 32\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.194 | Acc: 100.000% (128/128)\n0 79 Loss: 0.800 | Acc: 71.875% (92/128)\nEpoch: 32 | train acc: 98.4000015258789 | val acc: 66.9800033569336\n\nEpoch: 33\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.184 | Acc: 100.000% (128/128)\n0 79 Loss: 0.799 | Acc: 71.875% (92/128)\nEpoch: 33 | train acc: 99.4000015258789 | val acc: 67.0199966430664\n\nEpoch: 34\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.235 | Acc: 97.656% (125/128)\n0 79 Loss: 0.799 | Acc: 71.875% (92/128)\nEpoch: 34 | train acc: 98.5999984741211 | val acc: 67.02999877929688\n\nEpoch: 35\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.230 | Acc: 95.312% (122/128)\n0 79 Loss: 0.800 | Acc: 71.875% (92/128)\nSaving..\nEpoch: 35 | train acc: 98.0 | val acc: 67.13999938964844\n\nEpoch: 36\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.180 | Acc: 98.438% (126/128)\n0 79 Loss: 0.799 | Acc: 71.875% (92/128)\nEpoch: 36 | train acc: 98.80000305175781 | val acc: 67.05999755859375\n\nEpoch: 37\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.188 | Acc: 99.219% (127/128)\n0 79 Loss: 0.799 | Acc: 71.875% (92/128)\nEpoch: 37 | train acc: 99.0 | val acc: 67.06999969482422\n\nEpoch: 38\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.224 | Acc: 96.875% (124/128)\n0 79 Loss: 0.799 | Acc: 71.875% (92/128)\nEpoch: 38 | train acc: 98.5999984741211 | val acc: 67.11000061035156\n\nEpoch: 39\nLR head / body : [0.0001, 0.0001] ===\nUsing 500 data during training\n0 4 Loss: 0.184 | Acc: 98.438% (126/128)\n0 79 Loss: 0.799 | Acc: 71.875% (92/128)\nEpoch: 39 | train acc: 98.0 | val acc: 67.0999984741211\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# #Delete all files in results folder\n#import shutil, os\n#shutil.rmtree('/kaggle/working/results', ignore_errors=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:16:02.480943Z","iopub.status.idle":"2025-07-25T05:16:02.481135Z","shell.execute_reply.started":"2025-07-25T05:16:02.481039Z","shell.execute_reply":"2025-07-25T05:16:02.481048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}